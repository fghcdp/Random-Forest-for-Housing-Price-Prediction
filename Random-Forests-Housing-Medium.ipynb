{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santh\\AppData\\Local\\conda\\conda\\envs\\fastai\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Here we call the different libraries required. Fastai's library is utilized because a lot of the functions \n",
    "# that need to be used are simplified. Finally, two algorithms that we will look at, RandomForestRegressor &\n",
    "# GradientBoostingRegressor are also imported\n",
    "%matplotlib inline\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from IPython.display import display\n",
    "from sklearn import cross_validation, metrics\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \n",
    "PATH = \"data/housing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape  (1460, 81)\n",
      "test shape  (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing data are initialized\n",
    "train = pd.read_csv(f'{PATH}train.csv', low_memory = False)\n",
    "test = pd.read_csv(f'{PATH}test.csv', low_memory = False)\n",
    "print('train shape ', train.shape)\n",
    "print('test shape ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 80)\n"
     ]
    }
   ],
   "source": [
    "# We need to combine both training and testing data in one dataframe for pre-processing. The SalePrice column\n",
    "# needs to be dropped from the test data\n",
    "df_combined = pd.concat([train.drop('SalePrice', axis = 1), test])\n",
    "print(df_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find out the lotfrontage value with the highest occurence in each neighborhood\n",
    "def mode(df, key_cols, value_col, count_col):\n",
    "    return df.groupby(key_cols + [value_col]).size().to_frame(count_col).reset_index() \\\n",
    "             .sort_values(count_col, ascending=False).drop_duplicates(subset=key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame suitable for pre-processing\n",
    "def pre_proc_suitable(train, test):\n",
    "    # create dummy SalePrice column for the test data for now\n",
    "    test['SalePrice'] = 1\n",
    "    df_combined = pd.concat([train, test], ignore_index = True)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df, cols):\n",
    "    for x in range(len(cols)):\n",
    "        fills = mode(df, ['Neighborhood'], cols[x], 'count').reset_index(drop = True)\n",
    "        for y in range(len(df)):\n",
    "            Neigh = df.loc[y, 'Neighborhood']\n",
    "            if pd.isnull(df.loc[y, cols[x]]):\n",
    "                for z in range(len(fills)):\n",
    "                    if (fills.loc[z, 'Neighborhood'] == Neigh):\n",
    "                        df.loc[y, cols[x]] = fills.loc[z, cols[x]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function for pre-processing\n",
    "def pre_proc(df):\n",
    "    # Drop 'Id' column as it does not decide the SalePrice\n",
    "    df.drop(['Id'], axis = 1, inplace = True)\n",
    "    # Make new feature 'Age' of house\n",
    "    df['Age'] = df['YrSold'] - df['YearBuilt']\n",
    "    # Make new feature 'PeakSeason' when SalePrices are high\n",
    "    df['PeakSeason'] = df.MoSold.replace({1:0, 2:0, 3:0, 4:0, 5:1, 6:1, 7:1, 8:0, 9:0, 10:0, 11:0, 12:0})\n",
    "    # Convert MSSubClass from numerical datatype to string datatype\n",
    "    df.MSSubClass = df.MSSubClass.astype('object')\n",
    "    # Impute with values which occur must number of times in the neighborhood that the house is present\n",
    "    cols_to_impute = ['LotFrontage', 'MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1',\n",
    "                     'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenQual',\n",
    "                     'Functional', 'GarageCars', 'GarageArea']\n",
    "    df = impute(df, cols_to_impute)\n",
    "    # Change any string datatypes to categorical values\n",
    "    train_cats(df)\n",
    "    # Convert dataframe to numerical only by stripping the target column & converting dummy columns \n",
    "    df_proc, y, nas = proc_df(df, 'SalePrice', max_n_cat = 15)\n",
    "    return df_proc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the train and test data\n",
    "df_combined = pre_proc_suitable(train, test)\n",
    "# Perform pre-processing on the combined data\n",
    "processed_df, combined_y = pre_proc(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 294)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect back the pre-processed data into training and testing data using shape information from before\n",
    "df_train = processed_df.iloc[:1460, :]\n",
    "df_test = processed_df.iloc[1460:,:]\n",
    "y = combined_y[:1460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformation on SalePrice\n",
    "y = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((960, 294), (960,), (500, 294))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting Original Data into Validation and Training Set\n",
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 500  # Validation Set Size\n",
    "n_trn = len(df_train)-n_valid\n",
    "X_train, X_valid = split_vals(df_train, n_trn)\n",
    "y_train, y_valid = split_vals(y_train, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing accuracy and RMSE\n",
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A RandomForestRegressor with tuned parameters and cross validation performed on the training set\n",
    "rf = RandomForestRegressor(n_estimators=6000, n_jobs=-1, oob_score=True,  max_features = 0.5,\n",
    "                              min_samples_leaf = 2)\n",
    "rf.fit(X_train, y_train)\n",
    "print_score(rf)\n",
    "cv_score_rf = cross_validation.cross_val_score(rf, df_train, y, cv=5); cv_score_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cv_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.015, loss='ls', max_depth=15,\n",
       "             max_features=0.5, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=20, min_samples_split=0.001,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=4000,\n",
       "             presort='auto', random_state=500, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A GradientBoostingRegressor with tuned parameters and cross validation performed on the training set\n",
    "gbm = GradientBoostingRegressor(min_samples_split = 0.001, min_samples_leaf = 20, max_depth = 15, \n",
    "                                n_estimators = 4000, max_features = 0.5, subsample = 1.0, learning_rate = 0.015,\n",
    "                               random_state = 500)\n",
    "gbm.fit(X_train, y_train)\n",
    "cv_score_gbm = cross_validation.cross_val_score(gbm, df_train, y_train, cv=5); cv_score_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8970969921267689"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Highest score\n",
    "np.mean(cv_score_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 23)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping only important features from Random Forest Model\n",
    "rf_fi = rf_feat_importance(rf, df_train)\n",
    "to_keep_rf = rf_fi[rf_fi.imp > 0.005].cols\n",
    "df_train_rf_keep = df_train[to_keep_rf].copy()\n",
    "df_train_rf_keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.5, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=2, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=6000, n_jobs=-1,\n",
       "           oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_new = RandomForestRegressor(n_estimators=6000, n_jobs=-1, oob_score=True,  max_features = 0.5,\n",
    "                              min_samples_leaf = 2)\n",
    "rf_new.fit(X_train[to_keep_rf], y_train)\n",
    "cv_score_rf_new = cross_validation.cross_val_score(rf, df_train_rf_keep, y, cv=5); cv_score_rf_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881380839568029"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slight Improvement in RandomForest Score after removing features to reduce overfitting\n",
    "np.mean(cv_score_rf_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping only important features from Gradient Boosting Model\n",
    "gbm_fi = rf_feat_importance(gbm, df_train)\n",
    "to_keep_gbm = gbm_fi[gbm_fi.imp > 0.005].cols\n",
    "df_train_gbm_keep = df_train[to_keep_gbm].copy()\n",
    "df_train_gbm_keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.fit(X_train[to_keep_gbm], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87621, 0.87661, 0.87647, 0.88759, 0.85322])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score_gbm_new = cross_validation.cross_val_score(rf, df_train_gbm_keep, y, cv=5); cv_score_gbm_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8740197936377999"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't see an improvement here possibly because Gradient Boosting models sequentially work on those samples\n",
    "# that didn't fit well in previous trees, hence, all features are utilized more optimally in boosting\n",
    "np.mean(cv_score_gbm_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the first GBM model to predict the SalePrice of the pre-processed test data since it achieved the \n",
    "# highest accuracy of all models\n",
    "output = gbm.predict(df_test)\n",
    "output_antilog = np.exp(output)\n",
    "submission = pd.read_csv(f'{PATH}sample_submission.csv')\n",
    "saleP = pd.DataFrame(data = output_antilog, columns = ['SalePrice'])\n",
    "submission.SalePrice = saleP\n",
    "submission.to_csv('final.csv')\n",
    "# Remove First blank column before submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
